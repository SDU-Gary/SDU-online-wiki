---
title: 人工智能导论复习笔记
description: 
published: true
date: 2024-01-15T09:39:37.687Z
tags: 
editor: markdown
dateCreated: 2024-01-15T09:39:37.687Z
---

# 第一章

弱人工智能：指不能真正实现推理和解决问题的智能机器，这些机器表面看像是智能的，但是并不真正拥有智能，也不会有自主意识。
强人工智能：指真正能思维的智能机器，并且认为这样的机器是有知觉的和自我意识的，这类机器可分为类人（机器的思考和推理类似人的思维）与非类人（机器产生了和人完全不一样的知觉和意识，使用和人完全不一样的推理方式）两大类。
人工智能：用人工的方法在机器（计算机）上实现的智能，也称为机器智能。

# 第二章

## 2.1 知识与知识表示

### 知识的概念

**把有关信息关联在一起所形成的信息结构称为知识**。

### 知识的特性

- 相对正确性：任何知识都是在一定的条件及环境下产生的，在这种条件及环境下才是正确的。

- 不确定性：知识不止存在真假两种状态，还有许多中间的状态，也就是“真”的程度问题。

  - 随机性引起的不确定性：随机时间导致的
  - 模糊性引起的不确定性：
  - 经验引起的不确定性
  - 不完全性引起的不确定性：客观事物表现的不够充分

- 可表示性与可利用性

  知识可以用适当的形式表示出来。只是可以被利用，来解决问题

## 2.2一阶谓词逻辑表示法

### 命题

命题是一个非真即假的陈述句。

一个命题不能同时即为真又为假，但是可能在某种情况下为真，某种情况下为假。

命题逻辑表示法：无法把它所描述的事物的**结构**及**逻辑特征**反映出来，也不能把不同事物间的**共同特征**表述出来。

### 谓词

谓词由谓词名和个体组成。

谓词的一般形式：

![image-20240109165201377](https://s2.loli.net/2024/01/13/adzXWHK8oG6Vnpk.png)

P是谓词名，用于刻画个体的性质、状态或者个体之间的关系。一半大写字母开头。

x是个体，表示某个独立存在的事物或者某个抽象的概念。

谓词中的个体是变元，当每个变元都被具体的个体名字代替时，谓词就有了一个确定的真值：T或者F

**函数：**一半小写字母开头表示一个个体到另一个个体的映射。

**一阶谓词**：在谓词$P(x_1,x_2,...,x_n)$中，若$x_i(i=1,...,n)$都是**个体常量、变元或函数**则他是一元谓词，如果$x_i$本身又是一个谓词，那么他就是二阶谓词。

### 谓词公式

这里介绍了一些常见的否定、析取、合取、蕴含连接符，以及量词。不再赘述

**量词的辖域**：位于量词后面的单个谓词或者用括弧括起来的谓词公式。

**约束变元与自由变元**：辖域内与量词中同名的变元称为约束变元，不同名的变元称为自由变元。 

### 谓词公式的性质

- 永真性：如果谓词公式P对个体域D上的任何一个解释都取得真值T，则称P在D上是永真的；如果P在每个非空个体域上均永真，则称P永真
- 永假：如果谓词公式P对个体域D上的任何一个解释都取得真值F，则称P在D上是永假的；如果P在每个非空个体域上均永假，则称P永假。
- 可满足性和不可满足性：对于谓词公式P，如果至少存在一个解释使得P在此解释下的真值为T，则称P是可满足的，否则，则称P是不可满足的。



**等价性：**设P与Q是两个谓词公式，D是它们共同的个体域，若对D上的任何一个解释，P与Q都有相同的真值，则称公式P和Q在D上是等价的。如果D是任意个体域，则称P和Q是等价的，记为P$\Leftrightarrow$Q 。



**永真蕴含：**如果$P\rightarrow Q$永真，则称公式P永真蕴含$Q$，记作$P\Rightarrow Q$。称Q是P的逻辑结论，P为Q的前提。

**反证法：**Q为$P_1,P_2,...,P_n$的逻辑结论，当且仅当$(P_1\and P_2\and ...\and P_n)\and \neg Q$是不可满足的。



### 一阶谓词知识表示法

用谓词公式表示知识的一般步骤为

- 定义谓词及个体。
- 变元赋值。
- 用连接词连接各个谓词，形成谓词公式。

### 一阶谓词表示法的特点

优点：

- 自然性
- 精确性
- 严密性
- 容易实现

局限性：

- 不能表示不确定的知识
- 组合爆炸
- 效率低

## 2.3 产生式

### 产生式的表示

产生式分成四种

- 确定性规则知识的产生式表示
- 不确定性规则知识的产生式表示
- 确定性事实性知识的产生式表示
- 不确定性事实性知识的产生式表示



产生式与谓词逻辑中蕴含式的区别：形式基本相同。但是蕴含式知识产生式的特殊情况。除了逻辑蕴含之外，产生式包括各种操作、规则、变换、算子、函数。蕴含式只能表示确定性知识，产生式可以表示不确定性的知识。



### 产生式系统

一个产生是系统由规则库、控制系统、综合数据库三部分组成。

![image-20240109172609107](https://s2.loli.net/2024/01/13/AG1Q2Ekb5vrxPu8.png)

**规则库:** 用于描述相应领域内知识的产生式集合。 

 **综合数据库**(事实库、上下文、黑板等)：一个用于存放问题求解过程中各种当前信息的数据结构。 

 **控制系统（推理机构）**：由一组程序组成，负责整个产生式系统的运行，实现对问题的求解。 



产生式系统完成的工作：

- 推理：从规则库中选择与综合数据库中已知事实进行匹配
- 冲突消解：如果有多个匹配，需要选择一条执行
- 执行规则：如果某个规则的右部是结论，那么加入到综合数据库中。如果是操作就需要执行操作
- 检查推理终止条件。

### 产生式表示法特点

优点：

- 自然性
- 模块性
- 有效性 
- 清晰性 

缺点：

- 效率不高
- 不能表达结构性知识



适合产生式表示的知识：

- 领域知识间关系不密切，不存在结构关系。
- 经验性及不确定性的知识，且相关领域中对这些知识没有严格、统一的理论。
- 领域问题的求解过程可被表示为一系列相对独立的操作，且每个操作可被表示为一条或多条产生式规则。



## 2.4 框架表示法

框架是一种描述所述对象属性的数据结构。

一个框架由若干“**槽**”组成，每个槽划分成多个“**侧面**“。槽表示对象某一方面的属性，侧面表示相应属性的一个方面。

槽值可以是另外一个框架的名字。



特点

- 结构性
- 继承性
- 自然性

## 2.5 知识图谱

### 知识图谱的定义

知识图谱（Knowledge Graph/Vault），又称科学知识图谱，用各种不同的图形等可视化技术描述知识资源及其载体，挖掘、分析、构建、绘制和显示知识及它们之间的相互联系。

知识图谱是由一些相互连接的实体及其属性构成的

### 知识图谱的表示

使用三元组表示。两种形式

(实体1-关系-实体2)：中国-首都-北京
(实体-属性-属性值)：北京-人口-2069万

### 知识图谱的架构



逻辑结构

- 数据层：数据层主要是由一系列的事实组成，而知识以事实为单位进行存储。
- 模式层：模式层构建在数据层之上，是知识图谱的核心。



知识图谱的体系架构

![image-20240109174200237](https://s2.loli.net/2024/01/13/4FN5ryHodzVgTsU.png)

结构化数据：知识定义和标识都比较完备的数据。

半结构化数据：部分数据是结构化的，但存在大量结构化程度较低的数据

非结构化数据：没有定义和规范约束的“自由”数据

### 知识图谱构建

- 自顶向下构建：先为知识图谱定义好**本体与数据模式**，再将实体加入到知识库
- 自底向上构建：一些开放链接数据中提取出实体，选择其中置信度较高的加入到知识库，再构建顶层的**本体模式**

# 第三章

## 3.1 推理的基本概念

概念：人们在对各种事务进行分析、综合并做出最后决策时，通常是从已知事实出发，并运用已掌握的知识，找出其中蕴涵的事实，或归纳出新的事实。这一过程通常称为推理。

### 根据推出结论的途径来分

- 演绎推理：从全称判断推导出单称判断的过程，即由一般性知识推出适合于某一个具体情况的结论。这是一种从一般到个别的推理。使用三段论式：大前提、小前提、结论。
- 归纳推理：从足够多的实例中归纳出一般性的结论
- 默认推理：在知识不完全的情况下假设某些条件已经具备所进行的推理。

### 按推理时所用知识的确定性来划分

- 确定性推理：推理时所用的知识和证据都是确定的，结论也是确定的，其真值非真即假
- 不确定性推理：推理时使用的知识和证据不确定，结论也不确定。

### 按推出的结论与最终目标的距离来分

- 单调推理：随着推理的推进以及知识的加入，推出的结论越来越接近最终目标。演绎推理是单调推理。
- 非单调推理：在推理过程中由于新知识的加入，不仅没有加强已推出的结论，反而要否定他，使推理退回到前面的某一步，然后重新开始。默认推理是非单调推理。

### 按是否运用启发式知识来分

如果推理过程中使用了与推理有关的启发性知识，称为启发式推理，否则称为非启发式推理。



### 推理的方向

- 正向推理：从已知事实出发，在KB中找出当前可适用的知识，构成可适用知识集KS，然后按照某种冲突消解策略从KS中选出一条知识进行推理。并将新事实加入数据库中。然后从新的知识库中选出可用的知识进行推理。**盲目，效率低**

  ![image-20240113184958175](https://s2.loli.net/2024/01/13/ySRo9krb2vTO4Ei.png)

- 逆向推理：首先确定一个假设目标，然后寻找支持该假设的证据，若所需的证据都能找到，说明假设成立，如果找不到说明不成立。**若提出的假设目标不符合实际，会影响效率。**

  <img src="https://s2.loli.net/2024/01/13/VG8IgRbpTifow7D.png" alt="image-20240113185705609" style="zoom:50%;" />

- 混合推理：先正向退，帮助确定某个目标，然后逆向推理证实此目标。或者先逆向推理，然后根据的都的信息进行正向推理，推出更多结论。

- 双向推理：正向逆向同时进行，在中间碰头。



### 冲突消解策略

如果已知事实与知识库中的多个知识匹配，那么就需要决定使用哪个知识。

- 按规则的针对性排序：
- 按已知事实的新鲜性排序
- 按匹配度排序
- 按条件个数排序



## 3.2 自然演绎推理

从一组已知为真的事实出发，直接运用经典逻辑的推理规则推出结论的过程。

<img src="https://s2.loli.net/2024/01/13/udQSb9hYX1kaf38.png" alt="image-20240109214220437" style="zoom:50%;" />

<img src="https://s2.loli.net/2024/01/13/UiaHVwAqbKlGvd4.png" alt="image-20240109214235336" style="zoom:50%;" />

<img src="https://s2.loli.net/2024/01/13/uPsnKwOBE7DSbJH.png" alt="image-20240109214256154" style="zoom:50%;" />

## 3.3 谓词公式子集化

原子谓词公式是一个不能再分解的命题。

原子谓词公式及其否定，统称为文字。

任何文字的析取式称为子句。文字本身也是子句。

空子句不含有文字，是永假的，不可满足的。

谓词公式子集化是指将一个谓词公式化成一个相应的子句集

方法参考课本68页

## 3.4 鲁宾孙归结原理

子句集中的各个句子之间是合取关系。只要有一个子句不满足，那么整个子句集就不可满足。

检查子句集 S 中是否包含空子句，若包含，则 S 不可满足。
若不包含，在 S 中选择合适的子句进行归结，一旦归结出空子句，就说明 S 是不可满足的。

### 命题逻辑中的归结原理

**定义：设C1与C2是子句集中的任意两个子句，如果 C1中的文字L1与 C2中的文字L2互补，那么从C1和 C2中分别消去L1和L2，并将二个子句中余下的部分析取，构成一个新子句C12 。**

<img src="https://s2.loli.net/2024/01/13/IKYR5HPZL6gahlB.png" alt="image-20240109215516418" style="zoom:50%;" />

如上图，由C1和C2可以归结出C12，由C12和C3可以归结出C123。

考虑这个原理的合理性。在上面的例子中，由于子句之间是合取关系，所以C1与C2必须同时为真，最后结果才可能为真。如果C12为假，说明$\neg P$和$R$均为假，此时若要C1和C2分别为真，则等价于$Q$和$\neg Q$同时为真，不可能。所以最终子句集也为假。所以由C12假可以推出子句集为假。

称C1和C2为C12的亲本子句，C12为C1和C2的归结式。

**定义：如果亲本子句C1和C2均为真，那么归结式C12为真**

**推论：在进行归结之后，把归结式加入源子句集或者替换掉亲本子句，可以继续进行归结，此时新子句集的不可满足性与源子句集一致。**也就是说，一直归结下去，如果最终能归结到空子句，那么就说明不可满足。

### 谓词逻辑中的归结原理（参考课本P74)

谓词逻辑中有变元，所以不能直接消去互补文字，需要先用**最一般合一**进行代换。

![image-20240110144035334](https://s2.loli.net/2024/01/13/vxNPASGoWrjJq9V.png)

此外，如果参加归结的句子内部有可合一的文字，则在归结之前需要对这些文字先合一。

## 3.5 归结反演

利用归结原理证明定理的过程称为归结反演。

要证明$P_1P_2...P_n\rightarrow Q$只需要证明$(P_1\and P_2\and ... \and P_n)\and \neg Q$是不可满足的即可。

所以进行反演的过程可以总结如下：

（1）将已知前提表示为谓词公式*F*。

（2）将待证明的结论表示为谓词公式*Q*，并否定得到﹁ *Q* 。

（3）把谓词公式集$\{F,\neg Q\} $化为子句集*S*。

（4）应用归结原理对子句集*S*中的子句进行归结，并把每次归结得到的归结式都并入到*S*中。如此反复进行，若出现了空子句，则停止归结，此时就证明了*Q*为真。



## 3.6 应用归结原理解决问题

除了证明之外，归结原理还可以用来求解问题。例如如下的一个问题：

<img src="https://s2.loli.net/2024/01/13/916VXtu5NoQeC3r.png" alt="image-20240110151220673" style="zoom:50%;" />

利用归结原理求解问题的方式如下：

- 把已知前提($F_1,F_2,F_3$)用谓词公式表示出来，化为相应的子句集。设为S
- 把待求解的问题也用谓词公式表示出来，然后把他**否定**并于答案谓词$ANSWER$构成析取式。$ANSWER$是构造出来的用于求解答案的谓词公式。
- 把上面得到的谓词公式页表示为子句集，加入S中
- 对子句集应用归结原理进行归结
- 最终答案在ANSWER中

<img src="https://s2.loli.net/2024/01/13/Hx8RsBgctZhpTAL.png" alt="image-20240110151546745" style="zoom:50%;" />

<img src="https://s2.loli.net/2024/01/13/XGYnzVfIZKLyQNd.png" alt="image-20240110151556902" style="zoom:50%;" />

# 第四章

## 4.1 不确定性推理中的基本问题

不确定性推理：从不确定性的初始证据出发，通过运用不确定性的知识，最终推出具有一定程度的不确定性但却是合理或者近乎合理的结论的思维过程。

### 不确定性的表示与度量

- 知识不确定性的表示

  在专家系统中知识的不确定性一般是由领域专家给出的，通常是一个数值——知识的静态强度

- 证据不确定性的表示

  证据包括两种。一种是用户在求解问题时提供的初始证据。一种是由前面的结论作为后面的证据。证据的不确定性通常也由一个数值表示，他代表相应证据的不确定程度，叫做动态强度。

- 不确定性的度量

  ① 能充分表达相应知识及证据不确定性的程度。
  ② 度量范围的指定便于领域专家及用户对不确定性的估计。
  ③ 便于对不确定性的传递进行计算，而且对结论算出的不确定性量度不能超出度量规定的范围。
  ④ 度量的确定应当是直观的，同时应有相应的理论依据。 

### 不确定性匹配算法及阈值的选择

### 组合证据不确定性的算法

### 不确定性的传递算法

### 结论不确定性的合成



## 4.2 可信度方法

C-F模型：C-F模型是基于可信度表示的不确定性推理的基本方法，其他可信度方法都是在此基础上发展起来的。

可信度：根据经验对一个事物或现象为真的相信程度。

### 知识不确定性的表示

在C-F模型中，知识是用产生式规则表示的

<img src="https://s2.loli.net/2024/01/13/9riDzUo1bltRX4H.png" alt="image-20240110163520999" style="zoom:50%;" />

CF(H,E)的取值范围是[-1,1]，CF值为正，表示证据的出现增加了结论为真的可信度，值越大，证据越是支持结论为真。CF值为负，表示证据的出现增加了结论为假的可信度，值越小，证据越是支持结论为假。若是CF为0，则证据与结论无关。

CF值由专家直接给出

### 证据不确定性的表示

证据的不确定性也是用可信度因子表示。CF(E)=0.6表示E的可信度为0.6

静态强度CF(H,E):知识的强度，表示证据E为真时，结论H的可信程度

动态强度CF(E)表示证据E的不确定程度。

### 组合证据不确定性的算法

<img src="https://s2.loli.net/2024/01/13/cOH24YTXMBovsfk.png" alt="image-20240110164706047" style="zoom:50%;" />

### 不确定性传递算法

不确定性推理中，结论的可信度计算公式

$CF(H)=CF(H,E)\times max\{0,CF(E)\}$

### 结论不确定性的合成算法

若由多条不同知识推出了相同的结论，但是可信度不同，那么可以用合成算法求出综合可信度。

<img src="https://s2.loli.net/2024/01/13/ot3x2ORgKGAwlB7.png" alt="image-20240110183728088" style="zoom:50%;" />

（2） 使用如下公式求出$E_1$与$E_2$对H的综合影响所形成的可信度$CF_{1,2}(H)$

$CF_{1,2}(H)=\begin{cases}CF_1(H)+CF_2(H)-CF_1(H)CF_2(H)\ \ \ \  若CF_1(H)\ge0,CF_2(H)\ge0\\CF_1(H)+CF_2(H)+CF_1(H)CF_2(H)\ \ \ \ 若CF_1(H)<0,CF_2(H)<0\\ \frac{CF_1(H)+CF_2(H)}{1-min\{|CF_1(H)|,|CF_2(H)|\}}\ \ \ \ 若CF_1(H)CF_2(H)<0 \end{cases}$



## 4.3 证据理论

证据理论又称D-S理论，是一种处理不确定性的理论。

设 D 是变量 x 所有可能取值的集合，且 D 中的元素是互斥的，在任一时刻 x 都取且只能取 D 中的某一个元素为值，则称 D 为 x 的样本空间。

证据理论用集合表示命题，用D表示变量x的所有可能取值。则D为x的样本空间。D的任何一个子集A，都对应一个关于x的命题”x的值是在A中“。

### 概率分配函数

<img src="https://s2.loli.net/2024/01/14/jsIMln3F5xhqDpL.png" alt="image-20240110192726269" style="zoom:50%;" />

- $2^D$表示集合D的所有子集

- 概率分配函数的作用是把D的任意一个子集A都映射为$[0,1]$上的一个数$M(A)$。**概率分配函数实际上是对D的各个子集进行信任分配，$M(A)$表示分配给A的那一部分。**

  当A包含多个元素时，$M(A)$不包含对A的子集的信任度，而且也不知道如何对他进行分配。

- 概率分配函数与概率不同。



### 信任函数（下限函数）

 <img src="https://s2.loli.net/2024/01/14/zpd5iDboWRX72hg.png" alt="image-20240110193304909" style="zoom:50%;" />



### 似然函数（上限函数、不可驳斥函数）

Bel(A)表示的是对A为真的信任程度，Pl(A)表示的是对A为非假的信任程度。

<img src="https://s2.loli.net/2024/01/14/tWKTdEAB2hoS6In.png" alt="image-20240110193756855" style="zoom:50%;" />



### 概率分配函数的正交和（证据的组合）

对同样的证据，可能会得到两个不同的概率分配函数，那么就需要对他们进行组合。

设$M_1$和$M_2$是两个概率分配函数；则其正交和$M=M_1\oplus M_2$为:

$M(\empty)=0$

$M(A)=K^{-1}\sum\limits_{x\cap y=A}M_1(x)M_2(y)$

其中$K=1-\sum\limits_{x\cap y= \empty}M_1(x)M_2(y)=\sum\limits_{x\cap y\neq \empty}M_1(x)M_2(y)$

就是交集为A的集合的两两乘积除以相交集合的两两乘积和

### 基于不确定性的推理

参考课本P95~97



## 4.4 模糊推理方法

### 模糊集合的定义

- 论域：所讨论对象的全体
- 元素：论域中的每个对象
- 集合：论语中具有某种属性的确定的、可以彼此区别的元素的全体。

经典集合智能描述确定性的概念，模糊逻辑模仿人类的智慧，引入了隶属度的概念，描述介于真与假中间的过程。



**隶属函数：**模糊集合中每个元素被赋予了一个介于0和1之间的实数，描述其元素属于这个模糊集合的强度，该实数称为元素属于这个模糊集合的隶属函数。

### 模糊集合的表示法

$\mu_A(x)$表示x属于模糊集合A的隶属度。

- Zaden表示法：

  

  <img src="https://s2.loli.net/2024/01/14/lMAfFY78VxXOZ91.png" alt="image-20240110204356103" style="zoom:50%;" />

  这里的"/"只是一个分隔符。

- 序偶表示法

  ![image-20240110204427673](https://s2.loli.net/2024/01/14/MdJKLlThgn9CiFk.png)

- 向量表示法

  这里默认了集合中的元素依次是$x_1,x_2,...,x_n$

  ![image-20240110204437151](https://s2.loli.net/2024/01/14/k5eyJa2A9FiS8fQ.png)

### 隶属函数

模糊集合中所有元素的隶属度全体构成模糊集合的隶属函数

常见的模糊隶属函数有正态分布、三角分布、梯形分布。

隶属函数的确定方法包括

- 模糊统计法
- 专家经验发
- 二元对比排序法
- 基本概念扩充法

### 模糊集合的运算

<img src="https://s2.loli.net/2024/01/14/ZoWg9VyGX673fMl.png" alt="image-20240110205520848" style="zoom:50%;" />

<img src="./assets/image-20240110205535736.png" alt="image-20240110205535736" style="zoom:50%;" />

<img src="https://s2.loli.net/2024/01/14/TQm3VSMPkJfRpDF.png" alt="image-20240110205620794" style="zoom:50%;" />

### 模糊关系与模糊关系的合成

模糊关系是普通关系的推广，普通关系是描述两个集合中的元素之间是否有关联。模糊关系描述两个模糊集合中的元素之间的关联程度。当论域有限时，可以采用模糊矩阵表示模糊关系。

用$\mu_{A\times B}(a,b)$表示A中的a和B中的b之间的关系。那么有

$\mu_{A\times B}(a,b)=min\{\mu_A(a),\mu_B(b)\}$

若A，B为离散模糊集，那么他们的叉积运算为$\mu_{A\times B}=\mu_A^T\circ \mu_B$，最终得到模糊关系矩阵。



两个模糊关系矩阵，可以合成。模糊关系的合成方法：

- 最大-最小合成法：写出矩阵QR中的每个元素，然后将其中的乘积运算用取小运算代替，求和运算用取大运算代替
- 最大-代数积合成法：写出矩阵QR中的每个元素，然后将其中的求和运算用取大运算代替，乘积运算不变。

设模糊关系$Q\in X\times Y,R\in Y\times Z$,则模糊关系$S\in X\times Z$称为Q和R的合成。





### 模糊推理



模糊知识表示，对于模糊不确定性，一般采用隶属度来刻画。

模糊知识表示的一般形式是

(<对象>,<属性>,(<属性值>,<隶属度>))





### 模糊决策

模糊推理得到的是一个模糊向量，转化为确定值的过程称为"模糊决策".

模糊决策方法

- 最大隶属度法

  在模糊向量中，取隶属度最大的量作为推理结果。

  优点是简单易行，缺点是完全排除了其他隶属度较小的量的影响和作用，没有充分利用推理过程取得的信息。

- 加权平均判决法

  ![image-20240110215036801](https://s2.loli.net/2024/01/14/FPnWc36tLHubg1j.png)

- 中位数法

  取一个$u^*$使得$\sum\limits_{u_1}^{u^*}\mu(u_i)=\sum\limits_{u^*+1}^{u_n}\mu(u_i)$

  也就是，找一个位置，使得前面的隶属度之和等于后面的。如果这个中位数不是刚好分成两半，那就取左边或者右边都可以。

# 第五章

## 5.1 搜索中的基本问题与解决过程

### 搜索中要解决的问题

- 搜索过程是否一定能找到一个解
- 找到的是否是最优解
- 时间与空间复杂性如何
- 搜索过程是否会终止运行或者陷入一个死循环

### 搜索的主要过程

- 从初始或目的状态出发，并将它作为当前状态
- 扫描操作算子集，适用当前状态的一些操作算子作用于当前状态而得到新的状态，并建立指向其父结点的指针 。
- 检查所生成的新状态是否满足结束状态，如果满足，则得到问题的一个解，并可沿着有关指针从结束状态反向到达开始状态，给出一解答路径；否则，将新状态作为当前状态，返回第(2)步再进行搜索。 



### 搜索策略

- 盲目搜索：在不具有对于特定问题的任何有关信息的条件下，按固定的步骤（依次或者随机调用操作算子）进行搜索
- 启发式搜索：考虑特定问题领域可应用的知识，动态地确定调用操作算子的步骤，优先选择较适合的操作算子，尽量减少不必要的搜索，以求尽快地到达结束状态。
- 正向搜索：从初始状态出发的正向搜索，也称数据驱动。
- 逆向搜索：从目的状态出发的逆向搜索，也称目的驱动。



## 5.2 状态空间的搜索策略

### 状态空间表示法

状态空间是利用状态变量和操作符号，表示系统或问题的有关知识的符号体系，状态空间可以用一个四元组表示。

$(S,O,S_0,G)$

S表示状态集合，O表示操作算子的集合，$S_0$是包含问题的初始状态，G是包含问题的目的状态。



### 盲目的图搜索策略

- 回溯策略

  从初始状态出发，不停地、试探性的寻找路径，直到到达目的地或者“不可解节点”（死胡同）为止。当遇到不可解节点时就回溯到路径中最近的父节点上，产看从该点出发是否还有其他的子节点未被扩展，若有，则继续搜索，如果找到目标就成功退出搜索。

  回溯搜索需要维护三张表

  - PS：表示当前搜索路径上的状态
  - NPS：表示等待搜索的状态。也就是说，对于一个节点A的子节点集合，我们挑出一个来进行搜索，被搜索的放入PS中。剩余的被放入NPS中，等待之后用于搜索
  - NSS：表示找不到解路径的状态。

  如果一个状态出现在上面的任意一个表中，说明他已经被搜索过了，不用继续搜索。

- 宽度优先搜索：不断地按层扩展。需要维护两个表：open和closed。open中维护待搜索的状态，closed中维护已经被搜索过的状态。open是个队列。

- 深度优先搜索：沿着一个状态一直搜索下去，一个状态的所有子状态一定是先于字节的兄弟状态被搜索到。需要加深度限制值，否则可能一直搜索下去。



## 5.4 启发式图搜索策略

### 启发式信息

用来简化搜索过程有关具体问题领域的特性的信息叫做启发信息。

### 启发式策略

启发式策略就是利用与问题有关的启发进行引导搜索，从而对搜索进行剪枝。

运用启发式搜索的两种基本情况：

- 一个问题由于存在问题陈述和数据获取的模糊性，可能会使它没有一个确定的解。 
- 虽然一个问题可能有确定解，但是其状态空间特别大，搜索中生成扩展的状态数会随着搜索的深度呈指数级增长。

### 启发信息与估价函数

 启发信息按照运用方法的不同分为三种

- 陈述性启发信息：用于更准确、精炼的描述状态，使状态空间缩小
- 过程性启发信息：用于构造操作算子，使操作算子少而精
- 控制性启发信息：表示控制策略方面的知识，包括协调整个问题求解过程中所使用的各种处理方法。

按照作用分为三种

- 用于扩展节点的选择：用于决定先扩展哪一节点
- 用于生成节点的选择：用于决定生成哪些后继节点。
- 用于删除节点的选择：用于决定删除哪些节点。

**估价函数**用来估计待搜索节点的“有希望”程度，依次给他们排定次序。

估价函数$f(n)$定义为从初始节点经过节点n到目的节点的路径的最小代价估计值，一般形式是$f(n)=g(n)+h(n)$。g(n)是从初始节点到n节点的实际代价，$h(n)$是从n节点到目的节点的最佳路径的估计代价。

主要是$h(n)$体现了搜索的启发信息。

### A搜索算法

A搜索算法是将open表中的排序过程，改成了按照估价函数排序。

![image-20240111154409029](https://s2.loli.net/2024/01/14/nLutPD24cv7xRUz.png)

### A*算法

定义$h^*(n)$为状态n到目的状态的最优路径的代价，则当A搜索算法的启发函数$h(n)$小于等于$h^*(n)$时被称为A\*算法。也就是说$h(n)\le h^*(n)$

如果某一问题有解，那么利用A*搜索算法对该问题进行搜索则一定能搜索到解，并且一定能搜索到最优的解而结束。

A*算法有如下的性质

- 可采纳性

  一个搜索算法在最短路径存在时能保证找到它，就称该算法是可采纳的。

- 单调性

  <img src="https://s2.loli.net/2024/01/14/w9o6L1vM8szakXu.png" alt="image-20240111174014786" style="zoom:50%;" />

- 信息性

  在两个A*启发策略的$h_1$和$h_2$中，如果对于搜索空间中的任一状态n都有$h_1(n)\le h_2(n)$，就称$h_2$比$h_1$有更多的信息性，如果某一搜索策略的$h(n)$越大，则他所搜索的状态数要少得多。

# 第六章

智能计算：受自然界和生物界规律的启迪，人们根据其原理模仿设计了许多求解问题的算法，这些算法称为智能计算。

旨在通过模仿人类智能和认知过程，使计算机能够具备某种程度的智能和学习能力。

## 6.1 进化算法的产生与发展

### 进化算法的概念

进化算法(evolutionary algorithms，EA)是基于自然选择和自然遗传等生物进化机制的一种搜索算法。

### 进化算法的生物学背景

适者生存：最适合自然环境的个体产生后代的可能性大。

生物进化的基本过程：

<img src="https://s2.loli.net/2024/01/14/KQfVCsMivZ9eqnL.png" alt="image-20240111181933851" style="zoom:50%;" />

### 进化算法的设计原则

- 适用性原则：一个算法的适用性是指该算法所能适用的问题种类，它取决于算法所需的限制与假定
- 可靠性原则：算法的可靠性是指算法对于所设计的问题，以**适当的精度**求解其中大多数问题的能力
- 收敛性原则：是指算法是否能收敛到全局最优。在收敛的前提下，希望收敛速度尽量快。
- 稳定性原则：指算法对其控制参数及问题的数据的敏感度。
- 生物类比原则：生物界有效方法及操作可以通过类比的方法引入到算法中，有时会带来较好的结果

## 6.2 基本遗传算法

遗传算法：一类借鉴生物界自然选择和自然遗传机制的**随机搜索算法**,非常适用于处理传统搜索方法难以解决的复杂和非线性优化问题。

基本思想：**在求解问题时从多个解开始，然后通过一定的法则进行逐步迭代以产生新的解。**

### 编码

- 位串编码

  - 二进制编码：用若干二进制数表示一个个体，将原问题的解空间映射到位串空间 B={0，1}上，然后在位串空间上进行遗传操作。 

    优点：类似于生物染色体的组成，算法易于用生物遗传理论解释，遗传操作如交叉、变异等易实现；算法处理的模式数最多

  - Gray编码:将二进制编码通过一个变换进行转换得到的编码。 

- 实数编码

  采用实数表达法不必进行数制转换，可直接在解的表现型上进行遗传操作。

- 多参数映射编码

  把每个参数先进行二进制编码得到子串，再把这些子串连成一个完整的染色体

  

### 群体设定

初始种群的产生：随机产生、随机产生，选取最好的加入、把握最优解所占的范围，在此范围内设定初始群体



### 种群规模

群体中个体的数量称为种群规模

### 适应度函数

适应度函数是用来区分群里中的个体好坏的标准，是算法演化过程的驱动力，是进行自然选择的唯一依据。

在遗传算法中，将所有妨碍适应度值高的个体产生，从而影响遗传算法正常工作的问题统称为**欺骗问题**

### 适应度函数的尺度变换

- 线性变换
- 幂函数变换法
- 指数变换法



### 选择

选择操作也称为复制（reproduction）操作：从当前群体中按照一定概率选出优良的个体，使它们有机会作为父代繁殖下一代子孙。

判断个体优良与否的准则是各个个体的适应度值：个体适应度越高，其被选择的机会就越多。

选择方法包括以下几种

#### 个体选择概率分配法

- 适应度比例方法：各个个体被选择的概率与其适应度成正比
- 排序方法
  - 线性排序
  - 非线性排序

#### 选择个体方法

- 转盘赌选择
- 锦标赛选择方法
- 最佳个体保存法



### 交叉

两个生物机体配对或者复制时，染色体相互混合，产生一对由双方基因组成的新的染色体，称为**交叉**

基本的交叉方法包括：

- 一点交叉
- 二点交叉



### 变异

**变异**是指将编码中的一些位随机的变化

- 位点变异
- 逆转变异
- 插入变异
- 互换变异
- 移动变异



### 遗传算法的一般步骤

<img src="https://s2.loli.net/2024/01/14/4p3unPHciGbNhtI.png" alt="image-20240111192713022" style="zoom:50%;" />

## 6.3 改进遗传算法

### 双倍体遗传算法

采用隐形和显性两种基因同时进行进化，

交换的是基因。

### 双种群遗传算法

在遗传算法中使用多种群同时进化，并交换种群之间优秀个体所携带的遗传信息，以打破种群内的平衡态达到更高的平衡态，有利于算法跳出局部最优。

交换的是种群间的个体。

### 自适应遗传算法

当种群各个体适应度趋于一致或者趋于局部最优时，使 $P_c$,$P_m$增加，以跳出局部最优；而当群体适应度比较分散时，使$P_c$,$P_m$减少，以利于优良个体的生存。

## 6.5 群智能算法产生的背景

群体智能：由简单个体组成的群落与环境以及个体之间的互动行为。

**群智能算法**（swarm algorithms，SI）：受动物群体智能启发的算法。

## 6.6 粒子群算法

基本思想：将每个个体看作n维搜索空间中一个没有体积质量的粒子，在搜索空间中以一定的速度飞行，该速度决定粒子飞行的方向和距离。所有粒子有一个由优化函数决定的适应值。

基本原理：PSO初始化为一群随机粒子，然后通过迭代找到最优解。在每一次迭代中，粒子通过跟踪两个“极值”来更新自己。第一个就是粒子本身所找到的最优解，这个解称为个体极值。另个一是整个种群目前找到的最优解，这个解称为全局极值。



## 6.7 蚁群算法

### 基本思想

信息素跟踪：按照一定的概率沿着信息素较强的路径觅食

信息素遗留：会在走过的路上会释放信息素，使得在一定的范围内的其他蚂蚁能够觉察到并由此影响它们的行为。

# 第七章

## 7.2 专家系统的概念

### 专家系统的定义

专家系统是一种智能的计算机程序，它运用知识和推理来解决只有专家才能解决的复杂问题。

是一类包含知识和推理的智能计算机程序 

### 专家系统的特点

- 具有专家水平的专业知识
- 能进行有效的推理
- 具有启发性
- 具有灵活性
- 具有透明性
- 具有交互性



## 7.3 专家系统的工作原理

<img src="https://s2.loli.net/2024/01/14/oJmthN1ujDQdW69.png" alt="image-20240111214444161" style="zoom:50%;" />



专家系统的核心部分是推理机和知识库。其工作过程是根据知识库中的知识和用户提供的事实进行推理，不断的由已知的事实推出未知的结论即中间结果，并将中间结果保存到数据库中，作为已知的事实进行推理。从而把求解的问题从未知状态转换成已知状态。

知识库：主要用来存放领域专家提供的有关问题求解的专门知识。

推理机：模拟领域专家的思维过程，控制并执行对问题的求解。

## 7.4 知识获取的过程

<img src="https://s2.loli.net/2024/01/14/SraA7nVLm4UBJDR.png" alt="image-20240112132603961" style="zoom:50%;" />

## 7.5 机器学习

### 机器学习的基本概念

 机器学习：机器学习（Machine learning）使计算机能模拟人的学习行为，自动地通过学习来获取知识和技能，不断改善性能，实现自我完善。 

<img src="https://s2.loli.net/2024/01/14/jR6TKM1hxmaqpnB.png" alt="image-20240112132941616" style="zoom:50%;" />

### 机械式学习

又称记忆学习，或死记式学习：通过直接记忆或者存储外部环境所提供的信息达到学习的目的，并在以后通过对知识库的检索得到相应的知识直接用来求解问题。 

### 指导式学习

又称嘱咐式学习或教授式学习：由外部环境向系统提供一般性的指示或建议，系统把它们具体地转化为细节知识并送入知识库中。在学习过程中要反复对形成的知识进行评价，使其不断完善。

### 示例学习

也叫实例学习或从例子中学习 ：通过从环境中取得若干与某概念有关的例子，经归纳得出一般性概念的一种学习方法。

## 知识发现与数据挖掘

### 定义

含义：数据挖掘就是从数据库中挖掘知识，知识发现就是从数据库中发现知识。

数据挖掘是一种通过自动或半自动的方法从大量数据中发现有用信息的过程。

目的：从数据集中抽取和精化一般规律或模式

### 知识发现的一般过程

- 知识准备
- 数据挖掘
- 结果的解释和评估

### 知识发现的任务

- 数据总结
- 概念描述
- 分类
- 聚类
- 相关性分析
- 偏差分析
- 建模

### 知识发现的方法

- 统计方法
- 粗糙集
- 可视化
- 传统机器学习方法

## MYCIN系统结构

<img src="https://s2.loli.net/2024/01/14/VUWOz6NuLpgaIKQ.png" alt="image-20240114173058924" style="zoom:50%;" />

# 第八章

人工神经网络：模拟人脑神经系统的结构和功能，运用大量简单处理单元经广泛连接而组成的人工网络系统。



## 8.1 神经元与神经网络

### 神经网络的结构与工作方式

决定神经网络性能的三大要素

- 神经元的特性
- 神经元之间相互连接的方式——拓扑结构
- 为适应环境而改善性能的学习规则



神经网络的结构包括

- 前向型
- 反馈型



神经网络的工作方式包括

- 同步方式（并行方式）：任意时刻神经网络中的所有神经元调整状态
- 异步方式（串行方式）：任意时刻只有一个神经元调整状态，其他神经元不变



## 8.2 BP神经网络

### BP神经网络的结构

BP神经网络就是多层前向网络

设BP神经网络有m层，第一层为输入层，最后一层为输出层，中间各层称为隐层。



### 学习算法

**BP定理**：给定任意$\varepsilon > 0$，对于任意的连续函数，存在一个三层前向神经网络，它可以在任意$\varepsilon$平方误差精度内逼近连续函数

基本思想：它的基本思想是通过计算网络输出与期望输出之间的误差，并将误差从输出层反向传播到输入层，以调整网络的权重和偏置，从而最小化误差。通过反向传播计算每一层的误差贡献，并使用梯度下降法更新网络的权重和偏置。通过多次迭代，网络的输出逐渐接近期望输出，从而实现了对网络的训练。

  正向传播：输入信息由输入层传至隐层，最终在输出层输出。
 反向传播：修改各层神经元的权值，使误差信号最小。

<img src="https://s2.loli.net/2024/01/14/7poNflh1RkuDr6s.png" alt="image-20240112161752445" style="zoom:50%;" />

**层与层的连接是单向的，信息的传播是双向的。**

### 8.4 hopfield神经网络

Hopfield神经网络是一种基于反馈连接的无监督学习模型，它的基本思想是通过模拟神经元之间的相互作用来实现模式存储和模式识别的功能。

Hopfield神经网络由一组二值神经元（或称为节点）组成，每个神经元可以处于激活状态（取值为1）或非激活状态（取值为-1）。这些神经元之间通过全连接的权重矩阵进行连接，权重矩阵的对角线元素为0，表示神经元之间没有自我激活的连接。

Hopfield神经网络的特点是能够存储和恢复离散的二值模式。在存储模式时，网络会调整权重矩阵，使得存储的模式成为网络的吸引子。当网络受到部分破坏的模式作为输入时，它会通过迭代更新神经元状态的过程来恢复存储的模式。

只要连接权值构成的矩阵式非负对角元的对称矩阵，该网络就有串行稳定性；若该矩阵为非负定矩阵，就有并行稳定性。

## 8.6 卷积神经网络

### 深度学习

许多研究通过很多数学和工程技巧来增加神经网络隐层的层数，也就是深度，称为深度神经网络，相应的学习算法被称为深度学习。

深度学习是一种机器学习的分支，它通过模拟人脑神经网络的结构和功能，以及使用大规模数据进行训练，来实现对复杂模式和特征的学习和理解。深度学习的核心思想是构建深层次的神经网络模型，这些模型通过多个神经网络层级的堆叠来实现对数据的抽象表示和分层学习。



### 结构

CNN是一个多层的神经网络，每层由多个二维平面组成，而每个平面由多个独立神经元组成。
C层为特征提取层（卷积层）
S层是特征映射层（下采样层、池化层）。
CNN中的每一个C层都紧跟着一个S层。

<img src="https://s2.loli.net/2024/01/14/emsHKyavr4WjPhi.png" alt="image-20240112162853713" style="zoom:50%;" />

### 卷积

大部分的特征提取都依赖于卷积运算。利用卷积算子对图像进行滤波，可以得到显著的边缘特征。

在图像处理中，采用卷积运算对输入图像或者CNN上一层的特征图进行变换，也就是特征抽取，得到新的特征，卷积之后的结果称为特征图。

方法就是用卷积核在原矩阵上移动，然后求内积。移动方式是从左到右从上到下，每次移动s格（s为步幅）

### 池化

池化常用的方法：平均池化、最大池化

通过卷积获得了特征之后，如果直接利用这些特征训练分类器，计算量是非常大的。

对不同位置的特征进行**聚合统计**,称为池化 (pooling)。

卷积神经网络在池化层丢失大量的信息，从而降低了空间分辨率，导致了对于输入微小的变化，其输出几乎是不变的

### 胶囊网络

胶囊是一个包含多个神经元的载体，每个神经元表示了图像中出现的特定实体的各种属性。

胶囊不是传统神经网络中的一个神经元，而是一组神经元。

胶囊网络的核心思想：胶囊里封装的检测特征的相关信息是以向量的形式存在的，胶囊的输入是一个向量，是用一组神经元来表示多个特征。

## 8.7 生成对抗网络

### 判别式模型与生成式模型

深度学习模型分为判别式模型和生成式模型。

判别模型是将一个高维的感官输入映射为一个类别标签。

生成网络把随机点变成与数据集相似的图片。

### 生成对抗网络的结构与训练

GAN的两个相互交替的训练阶段：
●固定生成网络，训练判别网络
●固定判别网络，训练生成网络
两个网络相互对抗的过程，就是各自网络参数不断调整的过程，即学习过程。



# 人工智能未来发展趋势

- 大数据与合成数据的应用
- 大模型转向通用人工智能
- 自动驾驶
- AI物联网
- AI芯片的发展